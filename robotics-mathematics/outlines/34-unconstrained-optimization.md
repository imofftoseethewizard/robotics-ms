# Unconstrained Optimization: Gradient Descent, Newton's Method

I. Introduction (5 minutes)

1. Overview of the lecture
1. Goals of the lecture: understanding unconstrained optimization and its applications in robotics
1. Recap of the previous lecture: Bayesian estimation and applications in robotics

II. Unconstrained Optimization (5 minutes)

1. Definition and motivation
1. Examples of unconstrained optimization problems in robotics
1. Basic concepts: local minima, global minima, saddle points

III. Gradient Descent (15 minutes)

1. Derivation and intuition behind gradient descent
1. Step size and convergence properties
1. Applications in robotics: learning control policies, machine learning, trajectory optimization
1. Variants: stochastic gradient descent, momentum, adaptive step sizes

IV. Newton's Method (15 minutes)

1. Derivation and intuition behind Newton's method
1. Second-order approximation and Hessian matrix
1. Convergence properties and comparison with gradient descent
1. Applications in robotics: parameter estimation, model fitting, nonlinear optimization

V. Quasi-Newton Methods (10 minutes)

1. Motivation for quasi-Newton methods
1. Broyden–Fletcher–Goldfarb–Shanno (BFGS) method and its application in robotics
1. Limited-memory BFGS (L-BFGS) and its application in robotics

VI. Line Search and Trust Region Methods (5 minutes)

1. Line search methods: backtracking, Armijo rule
1. Trust region methods: trust region radius, update step
1. Applications in robotics

VII. Challenges and Limitations of Unconstrained Optimization (5 minutes)

1. Ill-conditioned problems
1. Non-convex optimization and local minima
1. Scalability and computational complexity

VIII. Conclusion (5 minutes)

1. Recap of the main points covered in the lecture
1. Importance of understanding and applying unconstrained optimization for robotics applications
1. Preview of the next lecture in the course: Constrained optimization: Lagrange multipliers, KKT conditions
